{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "arbitrary-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-blank",
   "metadata": {},
   "source": [
    "## Read training [data, data_won ] & Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minus-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 61, 61]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_srcs = ['./data/', './data_won']\n",
    "\n",
    "#################################################\n",
    "##############  read training data  #############\n",
    "\n",
    "# read training data\n",
    "wlan_dfs = []\n",
    "for wlan in ['wlan0', 'wlan1', 'wlan2']:\n",
    "    df_cat = None\n",
    "    for data_src in data_srcs:\n",
    "        df = pd.read_csv(f'{data_src}/{wlan.strip()}.csv')\n",
    "        df_cat = df if df is None else pd.concat([df_cat, df])\n",
    "    wlan_dfs.append(df_cat)\n",
    "\n",
    "#################################################\n",
    "##############  data preprocessing  #############\n",
    "\n",
    "# 取 exp : y = e^(-x/80)\n",
    "normalized_wlan_dfs = []\n",
    "for wlan_df in wlan_dfs:\n",
    "    sel_indexs = list(wlan_df.columns)[4:]\n",
    "    wlan_df = wlan_df.copy()\n",
    "    wlan_df[sel_indexs] = (-wlan_df[sel_indexs]/80)#.apply(np.exp)\n",
    "    normalized_wlan_dfs.append(wlan_df)\n",
    "\n",
    "# find clean mac list\n",
    "for i in range(3):\n",
    "    wlan_df = normalized_wlan_dfs[i]\n",
    "    \n",
    "    mac_list = []\n",
    "    for mac_, na_ in zip(wlan_df.columns[4:], wlan_df.isna().sum()[4:]):\n",
    "        if na_ < len(wlan_df)*0.60:\n",
    "            mac_list.append(mac_)\n",
    "\n",
    "mac_list = sorted(list(set(mac_list)))\n",
    "    \n",
    "# make clean mac df\n",
    "normalized_cleaned_wlan_dfs = []\n",
    "for i in range(3):\n",
    "    wlan_df = normalized_wlan_dfs[i]    \n",
    "    selec_col = ['x','y','z','timestamp'] + mac_list\n",
    "    df = pd.DataFrame(data={col:wlan_df[col] if col in wlan_df.columns else [np.nan] * len(wlan_df) for col in selec_col})\n",
    "    normalized_cleaned_wlan_dfs.append(df)\n",
    "\n",
    "# fill nan to 0\n",
    "for i in range(3):\n",
    "    normalized_cleaned_wlan_dfs[i] = normalized_cleaned_wlan_dfs[i].fillna(0)\n",
    "    \n",
    "[len(normalized_cleaned_wlan_dfs[i].columns) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "danish-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Info] Select Mac List Length: 57\n",
      "\t[Info] Mac List: ['00:0B:86:96:59:A0', '00:0B:86:96:59:A1', '00:0B:86:96:59:A2', '00:0B:86:96:60:80', '00:0B:86:96:60:81', '00:0B:86:96:60:82', '00:0B:86:96:61:C0', '00:0B:86:96:61:C1', '00:0B:86:96:61:C2', '00:0B:86:96:62:60', '00:0B:86:96:62:61', '00:0B:86:96:62:62', '00:0B:86:96:63:A0', '00:0B:86:96:69:00', '00:0B:86:96:69:01', '00:0B:86:96:69:02', '00:0B:86:96:6F:02', '00:0B:86:96:70:C2', '00:11:32:9D:2B:30', '00:11:32:9D:30:3A', '00:11:32:AD:8C:82', '00:11:32:AD:8E:B7', '00:11:32:B6:79:9E', '00:11:32:B6:7A:7F', '00:11:32:B6:86:00', '00:1A:1E:F3:67:A0', '04:8D:38:04:4E:86', '04:D4:C4:B5:E5:28', '0A:28:19:84:B7:DF', '0C:9D:92:02:8B:20', '0C:9D:92:02:8B:40', '16:11:32:9D:2B:30', '16:11:32:9D:30:3A', '16:11:32:B6:7A:7F', '16:11:32:B6:86:00', '40:B0:76:34:03:48', '44:A5:6E:41:9B:BA', '4A:E2:44:27:58:F2', '6C:F3:7F:31:FA:20', '6C:F3:7F:31:FA:21', '6C:F3:7F:31:FA:22', '6C:F3:7F:3A:B7:B0', '6C:F3:7F:3A:B7:B1', '6C:F3:7F:3A:B7:B2', '6C:F3:7F:3A:C2:90', '6C:F3:7F:3A:C2:91', '6C:F3:7F:3A:C2:92', '86:2A:FD:78:C5:90', '8C:3B:AD:22:02:66', '90:9A:4A:09:41:63', 'AA:9A:4A:09:41:65', 'AA:C9:E3:4B:6C:91', 'B0:6E:BF:3E:C6:B8', 'B8:A3:86:57:17:67', 'C0:C9:E3:4B:6C:9F', 'D4:5D:64:E0:9F:88', 'DC:FB:02:61:9E:31']\n"
     ]
    }
   ],
   "source": [
    "print(f'\\t[Info] Select Mac List Length: {len(mac_list)}')\n",
    "print(f'\\t[Info] Mac List: {mac_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-ethics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-contamination",
   "metadata": {},
   "source": [
    "## Read Testing [data_won_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suited-display",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 61, 61]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_srcs = ['./data_won_test']\n",
    "\n",
    "#################################################\n",
    "##############  read training data  #############\n",
    "\n",
    "# read training data\n",
    "df = pd.read_csv(f'./data_won_test/wlan012.csv')\n",
    "wlan_dfs_test = []\n",
    "for wlan in ['wlan0', 'wlan1', 'wlan2']:\n",
    "    wlan_dfs_test.append( df.loc[df['name']==f'{wlan}'].iloc[:, 2:] )\n",
    "\n",
    "#################################################\n",
    "##############  data preprocessing  #############\n",
    "\n",
    "# 取 exp : y = e^(-x/80)\n",
    "normalized_wlan_dfs_test = []\n",
    "for wlan_df in wlan_dfs_test:\n",
    "    sel_indexs = list(wlan_df.columns)[4:]\n",
    "    wlan_df = wlan_df.copy()\n",
    "    wlan_df[sel_indexs] = (-wlan_df[sel_indexs]/80)#.apply(np.exp)\n",
    "    normalized_wlan_dfs_test.append(wlan_df)\n",
    "\n",
    "# # find clean mac list\n",
    "# for i in range(3):\n",
    "#     wlan_df = normalized_wlan_dfs[i]\n",
    "    \n",
    "#     mac_list = []\n",
    "#     for mac_, na_ in zip(wlan_df.columns[4:], wlan_df.isna().sum()[4:]):\n",
    "#         if na_ < len(wlan_df)*0.40:\n",
    "#             mac_list.append(mac_)\n",
    "\n",
    "# mac_list = sorted(list(set(mac_list)))\n",
    "    \n",
    "# make clean mac df\n",
    "normalized_cleaned_wlan_dfs_test = []\n",
    "for i in range(3):\n",
    "    wlan_df = normalized_wlan_dfs_test[i]    \n",
    "    selec_col = ['x','y','z','timestamp'] + mac_list\n",
    "    df = pd.DataFrame(data={col:wlan_df[col] if col in wlan_df.columns else [np.nan] * len(wlan_df) for col in selec_col})\n",
    "    normalized_cleaned_wlan_dfs_test.append(df)\n",
    "\n",
    "# fill nan to 0\n",
    "for i in range(3):\n",
    "    normalized_cleaned_wlan_dfs_test[i] = normalized_cleaned_wlan_dfs_test[i].fillna(0)\n",
    "    \n",
    "[len(normalized_cleaned_wlan_dfs_test[i].columns) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-essence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-helmet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-apartment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-sudan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-klein",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-doctor",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Wlan0,1,2並起來, Random Training data 5x5x5 or 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deadly-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################################################################\n",
    "########################## Train & Valid ##########################\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "# 5x5x5\n",
    "for idx1, row1 in normalized_cleaned_wlan_dfs[0].iterrows():\n",
    "    x1 = row1.values[0]\n",
    "    y1 = row1.values[1]\n",
    "    signals1 = row1.values[4:]\n",
    "    for idx2, row2 in normalized_cleaned_wlan_dfs[1].iterrows():\n",
    "        x2 = row2.values[0]\n",
    "        y2 = row2.values[1]\n",
    "        signals2 = row2.values[4:]\n",
    "        if x2!=x1 or y2!=y1:\n",
    "            continue\n",
    "        else:\n",
    "            for idx3, row3 in normalized_cleaned_wlan_dfs[2].iterrows():\n",
    "                x3 = row3.values[0]\n",
    "                y3 = row3.values[1]\n",
    "                signals3 = row3.values[4:]\n",
    "                if x3!=x2 or y3!=y2:\n",
    "                    continue\n",
    "                else:\n",
    "                    X.append(np.concatenate([signals1,signals2,signals3]))\n",
    "                    Y.append([x1,y1])\n",
    "\n",
    "#                     normalized_cleaned_wlan_dfs[2].drop(idx3, inplace=True)\n",
    "#                     break\n",
    "#             normalized_cleaned_wlan_dfs[1].drop(idx2, inplace=True)\n",
    "#             break\n",
    "\n",
    "X, Y = np.stack(X), np.stack(Y)\n",
    "\n",
    "###################################################################\n",
    "############################## Test ###############################\n",
    "\n",
    "X_test, Y_test = [], []\n",
    "\n",
    "# 5*5*5\n",
    "for idx1, row1 in normalized_cleaned_wlan_dfs_test[0].iterrows():\n",
    "    x1 = row1.values[0]\n",
    "    y1 = row1.values[1]\n",
    "    signals1 = row1.values[4:]\n",
    "    for idx2, row2 in normalized_cleaned_wlan_dfs_test[1].iterrows():\n",
    "        x2 = row2.values[0]\n",
    "        y2 = row2.values[1]\n",
    "        signals2 = row2.values[4:]\n",
    "        if x2!=x1 or y2!=y1:\n",
    "            continue\n",
    "        else:\n",
    "            for idx3, row3 in normalized_cleaned_wlan_dfs_test[2].iterrows():\n",
    "                x3 = row3.values[0]\n",
    "                y3 = row3.values[1]\n",
    "                signals3 = row3.values[4:]\n",
    "                if x3!=x2 or y3!=y2:\n",
    "                    continue\n",
    "                else:\n",
    "                    X_test.append(np.concatenate([signals1,signals2,signals3]))\n",
    "                    Y_test.append([x1,y1])\n",
    "\n",
    "#                     normalized_cleaned_wlan_dfs_test[2].drop(idx3, inplace=True)\n",
    "#                     break\n",
    "#             normalized_cleaned_wlan_dfs_test[1].drop(idx2, inplace=True)\n",
    "#             break\n",
    "\n",
    "X_test, Y_test = np.stack(X_test), np.stack(Y_test)\n",
    "\n",
    "with open('./ckpt/dataset.pkl', 'wb') as fp:\n",
    "    pickle.dump({'mac_list': mac_list, 'x': X, 'y': Y, 'x_test': X_test, 'y_test': Y_test}, fp, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "twelve-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./ckpt/dataset.pkl', 'rb') as fp:\n",
    "    dic = pickle.load(fp)\n",
    "    mac_list, X, Y, X_test, Y_test = dic['mac_list'], dic['x'], dic['y'], dic['x_test'], dic['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latin-pitch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, (25119, 171), (25119, 2), (5088, 171), (5088, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mac_list), X.shape, Y.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-marshall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-master",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-render",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gothic-compound",
   "metadata": {},
   "source": [
    "## Split Train & Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "equipped-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_train_test_split(X, Y, mode='sklearn'):\n",
    "    test_size = 0.10\n",
    "    if mode=='sklearn':\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "    elif mode=='mask':\n",
    "        point_set = np.unique(Y, axis=0)\n",
    "        train_point_set = point_set[int(np.ceil(len(point_set)*test_size)):]\n",
    "        valid_point_set = point_set[:int(np.ceil(len(point_set)*test_size))]\n",
    "        X_train, X_test, Y_train, Y_test = [], [], [], []\n",
    "        for x, y in zip(X, Y):\n",
    "            find = False\n",
    "            for point in train_point_set:\n",
    "                if sum(np.abs(y-point)) == 0:\n",
    "                    X_train.append(x)\n",
    "                    Y_train.append(y)\n",
    "                    find = True\n",
    "            if find:\n",
    "                continue\n",
    "            else:\n",
    "                X_test.append(x)\n",
    "                Y_test.append(y)\n",
    "        X_train, X_test, Y_train, Y_test = np.stack(X_train), np.stack(X_test), np.stack(Y_train), np.stack(Y_test)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "underlying-methodology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25119, 171), (25119, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "impressed-slide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.    , 0.    , 0.95  , ..., 0.    , 0.    , 0.7625],\n",
       "       [0.    , 0.    , 0.95  , ..., 0.8875, 0.9125, 0.7875],\n",
       "       [0.    , 0.    , 0.95  , ..., 0.8875, 0.9125, 0.7625],\n",
       "       ...,\n",
       "       [0.    , 0.    , 0.    , ..., 0.    , 0.8875, 0.7375],\n",
       "       [0.    , 0.    , 0.    , ..., 1.0375, 0.8875, 0.7375],\n",
       "       [0.    , 0.    , 0.    , ..., 0.8625, 0.8875, 0.7625]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-findings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-return",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prepared-forum",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "quick-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=64, out_dim=2):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(in_dim, hid_dim, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "            \n",
    "#             nn.Dropout(0.5),\n",
    "            nn.Linear(hid_dim, hid_dim, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "            \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hid_dim, hid_dim, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.BatchNorm1d(hid_dim),\n",
    "                        \n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hid_dim, out_dim, bias=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-interview",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "novel-thriller",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Run(3eh8rfr0)</h1><iframe src=\"https://wandb.ai/wubinary/Iot_wifi_location/runs/3eh8rfr0\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc514b7c080>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, wandb\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "wandb.init(project='Iot_wifi_location', name='mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "portuguese-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[500/500]  \r"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def weighted_mse_loss(input, target, weight):\n",
    "    weight = weight.to(target.device)\n",
    "    return torch.sum(weight * (input - target) ** 2)\n",
    "\n",
    "class Averager():\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.n = 0\n",
    "    def push(self, a):\n",
    "        self.sum += a\n",
    "        self.n += 1\n",
    "    def avg(self):\n",
    "        return self.sum/(self.n+1e-8)\n",
    "\n",
    "def train():\n",
    "    global steps\n",
    "    model.train()\n",
    "    \n",
    "    losses = Averager()\n",
    "    for x, y_ in train_loader:\n",
    "        x, y_ = x.cuda(), y_.cuda()\n",
    "        y = model(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(y, y_)\n",
    "#         loss = weighted_mse_loss(y, y_, torch.tensor([1.0, 1.0]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps += 1\n",
    "        losses.push(loss.item())\n",
    "        wandb.log({'train_mse':loss.item()})\n",
    "    return losses.avg()\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid():\n",
    "    model.eval()\n",
    "    \n",
    "    losses = Averager()\n",
    "    for x, y_ in valid_loader:    \n",
    "        x, y_ = x.cuda(), y_.cuda()    \n",
    "        y = model(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(y, y_)   \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.push(loss.item())\n",
    "    wandb.log({'valid_mse':losses.avg()})\n",
    "    return losses.avg()\n",
    "    \n",
    "# X_train, X_valid, Y_train, Y_valid = make_train_test_split(X, Y, mode='sklearn') # mode: sklearn, mask\n",
    "# X_train, Y_train, X_valid, Y_valid = torch.Tensor(X_train), torch.Tensor(Y_train),\\\n",
    "#                                     torch.Tensor(X_valid), torch.Tensor(Y_valid)\n",
    "X_train, Y_train, X_valid, Y_valid = torch.Tensor(X), torch.Tensor(Y), torch.Tensor(X_test), torch.Tensor(Y_test)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "valid_dataset = TensorDataset(X_valid, Y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=256,\n",
    "                          drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size=256)\n",
    "\n",
    "model = Model(in_dim=X.shape[1])\n",
    "model.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "\n",
    "steps = 0\n",
    "max_steps = 100000\n",
    "max_epochs = 500 \n",
    "best_valid_loss = np.inf\n",
    "for epoch in range(max_epochs):\n",
    "    print(f'\\t[{epoch+1}/{max_epochs}]', end='  \\r')\n",
    "    \n",
    "    train_loss = train()\n",
    "    valid_loss = valid()\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save({'mse':valid_loss ,'model':best_model.state_dict()}, './ckpt/best.pt')\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "#     print(f'\\t[{steps}/{max_steps}]', end='  \\r')\n",
    "#     if steps > max_steps:\n",
    "#         break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-valuable",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-diary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-cherry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "guilty-friend",
   "metadata": {},
   "source": [
    "## Test_won_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "activated-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_srcs = ['./data_won']#, './data']\n",
    "\n",
    "# #################################################\n",
    "# ##############  read training data  #############\n",
    "\n",
    "# # read training data\n",
    "# wlan_dfs = []\n",
    "# for wlan in ['wlan0', 'wlan1', 'wlan2']:\n",
    "#     df_cat = None\n",
    "#     for data_src in data_srcs:\n",
    "#         df = pd.read_csv(f'{data_src}/{wlan.strip()}.csv')\n",
    "#         df_cat = df if df is None else pd.concat([df_cat, df])\n",
    "#     wlan_dfs.append(df_cat)\n",
    "\n",
    "# #################################################\n",
    "# ##############  data preprocessing  #############\n",
    "\n",
    "# # 取 exp : y = e^(-x/80)\n",
    "# normalized_wlan_dfs = []\n",
    "# for wlan_df in wlan_dfs:\n",
    "#     sel_indexs = list(wlan_df.columns)[4:]\n",
    "#     wlan_df = wlan_df.copy()\n",
    "#     wlan_df[sel_indexs] = (-wlan_df[sel_indexs]/80)#.apply(np.exp)\n",
    "#     normalized_wlan_dfs.append(wlan_df)\n",
    "\n",
    "# # find clean mac list\n",
    "# # for i in range(3):\n",
    "# #     wlan_df = normalized_wlan_dfs[i]\n",
    "    \n",
    "# #     mac_list = []\n",
    "# #     for mac_, na_ in zip(wlan_df.columns[4:], wlan_df.isna().sum()[4:]):\n",
    "# #         if na_ < len(wlan_df)*0.40:\n",
    "# #             mac_list.append(mac_)\n",
    "\n",
    "# # mac_list = sorted(list(set(mac_list)))\n",
    "\n",
    "# # make clean mac df\n",
    "# normalized_cleaned_wlan_dfs = []\n",
    "# for i in range(3):\n",
    "#     wlan_df = normalized_wlan_dfs[i]    \n",
    "#     selec_col = ['x','y','z','timestamp'] + mac_list\n",
    "#     df = pd.DataFrame(data={col:wlan_df[col] if col in wlan_df.columns else [np.nan] * len(wlan_df) for col in selec_col})\n",
    "#     normalized_cleaned_wlan_dfs.append(df)\n",
    "\n",
    "\n",
    "# # fill nan to 0\n",
    "# for i in range(3):\n",
    "#     normalized_cleaned_wlan_dfs[i] = normalized_cleaned_wlan_dfs[i].fillna(0)\n",
    "    \n",
    "\n",
    "# ############################################################################\n",
    "# ############################################################################\n",
    "\n",
    "# X, Y = [], []\n",
    "\n",
    "# # 5x5x5\n",
    "# for idx1, row1 in normalized_cleaned_wlan_dfs[0].iterrows():\n",
    "#     x1 = row1.values[0]\n",
    "#     y1 = row1.values[1]\n",
    "#     signals1 = row1.values[4:]\n",
    "#     for idx2, row2 in normalized_cleaned_wlan_dfs[1].iterrows():\n",
    "#         x2 = row2.values[0]\n",
    "#         y2 = row2.values[1]\n",
    "#         signals2 = row2.values[4:]\n",
    "#         if x2!=x1 or y2!=y1:\n",
    "#             continue\n",
    "#         else:\n",
    "#             for idx3, row3 in normalized_cleaned_wlan_dfs[2].iterrows():\n",
    "#                 x3 = row3.values[0]\n",
    "#                 y3 = row3.values[1]\n",
    "#                 signals3 = row3.values[4:]\n",
    "#                 if x3!=x2 or y3!=y2:\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     X.append(np.concatenate([signals1,signals2,signals3]))\n",
    "#                     Y.append([x1,y1])\n",
    "\n",
    "# #                     normalized_cleaned_wlan_dfs[2].drop(idx3, inplace=True)\n",
    "# #                     break\n",
    "# #             normalized_cleaned_wlan_dfs[1].drop(idx2, inplace=True)\n",
    "# #             break\n",
    "\n",
    "# X_test, Y_test = np.stack(X), np.stack(Y)\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "#X_test, Y_test = torch.tensor(X_test), torch.tensor(Y_test)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(Y_test))\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "transsexual-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 6.779625807306959  mse_x: 5.179384355307586  mse_y: 8.379867259306332  ;   std_x: 2.0091824531555176  std_y 0.4653508961200714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mse, n = 0, 0\n",
    "model.eval()\n",
    "pred, answ = [], []\n",
    "for x, y_ in test_loader:\n",
    "    y = best_model(x.cuda().float())\n",
    "    \n",
    "    pred.append(y.cpu().detach())\n",
    "    answ.append(y_.cpu().detach())\n",
    "\n",
    "pred = torch.cat(pred)\n",
    "answ = torch.cat(answ)\n",
    "    \n",
    "# mse \n",
    "mse = torch.nn.MSELoss()(pred, answ)\n",
    "mse_x = torch.nn.MSELoss()(pred[:,0], answ[:,0])\n",
    "mse_y = torch.nn.MSELoss()(pred[:,1], answ[:,1])\n",
    "std_x = torch.std(pred[:,0])\n",
    "std_y = torch.std(pred[:,1])\n",
    "print('mse:',float(mse), ' mse_x:',float(mse_x), ' mse_y:', float(mse_y), ' ; ', ' std_x:', float(std_x), ' std_y', float(std_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "architectural-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5088)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_aug = []\n",
    "answ_aug = []\n",
    "x,y = 0,0\n",
    "tmp = []\n",
    "for i in range(len(answ)):\n",
    "    x_, y_ = answ[i]\n",
    "    if x==x_ and y==y_:\n",
    "        tmp.append(pred[i])\n",
    "    else:\n",
    "        if len(tmp)!=0:\n",
    "            pred_aug.append(torch.stack(tmp).mean(axis=0))\n",
    "            answ_aug.append(ans)\n",
    "        tmp = [pred[i]]\n",
    "        ans = answ[i]\n",
    "        x, y = x_, y_\n",
    "        \n",
    "pred_aug = torch.stack(pred_aug)\n",
    "answ_aug = torch.stack(answ_aug)\n",
    "\n",
    "len(pred_aug), len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "organized-capitol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 4.882162972777902  mse_x: 4.109006375826632  mse_y: 5.655319569729171  ;   std_x: 1.9143109321594238  std_y 0.47439807653427124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mse = torch.nn.MSELoss()(pred_aug, answ_aug)\n",
    "mse_x = torch.nn.MSELoss()(pred_aug[:,0], answ_aug[:,0])\n",
    "mse_y = torch.nn.MSELoss()(pred_aug[:,1], answ_aug[:,1])\n",
    "std_x = torch.std(pred_aug[:,0])\n",
    "std_y = torch.std(pred_aug[:,1])\n",
    "print('mse:',float(mse), ' mse_x:',float(mse_x), ' mse_y:', float(mse_y), ' ; ', ' std_x:', float(std_x), ' std_y', float(std_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "younger-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'mse':float(mse) ,'model':best_model.state_dict()}, './ckpt/final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-commissioner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "criminal-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.67199  3.473061] [9.13  2.436]\n",
      "[9.109683  3.5988626] [9.235 5.876]\n",
      "[10.507136   2.5335171] [13.466 -0.19 ]\n",
      "[11.74102   3.457102] [14.37  7.15]\n",
      "[10.343455   3.0551696] [8.36  7.122]\n",
      "[5.806749  3.5191298] [4.17 5.4 ]\n",
      "[6.7405457 3.3443322] [3.725 1.78 ]\n",
      "[6.7706885 2.720373 ] [6.63 6.31]\n",
      "[7.0206285 3.458142 ] [3.9  3.49]\n",
      "[6.9714355 3.3399568] [4.5  6.07]\n",
      "[7.1470594 3.3458545] [6.974 4.831]\n",
      "[11.004126   2.6783493] [11.771  7.524]\n",
      "[6.8950634 3.5028985] [6.85  2.555]\n",
      "[6.623708 3.220631] [2.624 0.294]\n",
      "[9.143018  2.3294158] [11.783  2.433]\n",
      "[10.338418   2.7148223] [7.94  3.218]\n",
      "[7.4819956 3.2415762] [6.695 4.186]\n",
      "[6.4124923 3.541331 ] [4.43 3.25]\n",
      "[9.321188  3.4856868] [8.907 4.939]\n",
      "[11.33448   4.442448] [10.579  2.339]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(answ_aug)):\n",
    "    print(pred_aug[i].numpy(), answ_aug[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-teddy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-instrument",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-showcase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-beauty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-tract",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-parker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-singing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-contrary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-chick",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-waste",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-landscape",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
